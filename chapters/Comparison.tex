\label{chapter:discussion}

It is worth pointing out that in the gauge potentials discussed for both moving media and dynamic modulations, all operations on the photon modes belong to the group of complex unitary ($U^\dagger = U^{-1}$) $2 \times 2$ matrices with determinant $1$, known as special unitary $SU(2)$. However, the gauge transformation itself is given by setting a single phase factor between both states $\ket{1}$ and $\ket{2}$. Since there is only a single phase difference that can be set, the gauge degree of freedom belongs to the group of complex unitary $1 \times 1$ matrices, $U(1)$. This is identical to the gauge symmetry of the magnetic vector potential in electromagnetism \cite{Sakurai1995}.

\section{Feasibility of modulation strengths}

Here, the feasibility of achieving such modulation strengths on a waveguide are discussed. Current state of the art silicon modulators can achieve modulation frequencies upwards of $20 GHz$, at a modulation strength of $\delta/\epsilon = 5 \times 10^{-4}$ \cite{Reed2010}. In previous simulations, a significantly larger modulation strength was used ($\delta/\epsilon = 0.1$). The choice of the stronger modulation strength was two-fold. On one hand, a stronger modulation corresponds directly to a shorter coherence length, allowing the simulation to be far smaller than otherwise practical. Likewise, the response of the mode transitions discussed here are purely linear, that is, the modal transition is independent of the amplitude and phase of the incident light. Owing to this linearity, the photonic AB isolator can be scaled to effectively any frequency within the given constraints. Previous works \cite{Fang2013c} have demonstrated such a photonic AB effect at radio frequencies, and have a possibility of scaling to the gigahertz frequency range. However, scaling both the waveguides and the modulation to optical frequencies is experimentally demanding. These permittivity modulations in silicon operate by applying an electric field to a material, resulting in an induced birefringence \cite{Reed2010}.

On top of the weak modulation strength, silicon modulators also suffer from \textit{insertion loss}, which takes into account the amount of optical power that is lost as a result of adding the component to the circuit. These insertion losses scale inversely with device size, as a result of larger interactions with sidewall roughness (a characteristic of the light-matter interaction), however several recent works have noted a significant minimisation of these effects \cite{Gao2006,Ren1992}. The dominance of silicon in integrated circuits is unfortunate for photonics, as silicon is inherently restricted as an optical material for the purposes of modulation. This is because the crystal structure of silicon (centrosymmetric) ensures that the electro-optic modulation can never be linear in nature.

Throughout this thesis, the frequencies of light have been in the near-infrared to microwave regime. The ideal optical isolator would possess an infinite operating bandwidth operating at a high contrast ratio, whilst commonly used magneto-optic isolators operate anywhere between the visible and microwave spectrum. To remain both in the micrometer scale while inducing photonic transitions, modulation strengths of upwards of $\delta / \epsilon = 0.1$ are necessary.

\section{Comparisons between FDTD and FDFD}

The unique aspect of the FDFD simulation implemented in this thesis is that it can naturally represent electric field profiles and amplitudes at any sideband frequency, without any of the post-processing that would be typically required in time-domain simulations. In the time-domain simulations, obtaining Fourier transform puts further strain on the simulation. This is because the field must be sampled at every time-point - a relatively costly process. Although the Fourier transform can be split into chunks to reduce the memory footprint, field values must still be extracted. On the other hand, FDFD solves for the steady-state in a single calculation (the matrix inversion), and the frequencies can be simply extracted from the field matrix. 

However, the matrix inversion on the wave matrix $A$ requires specialised numeric solvers. This is because the wave matrix typically exceeds $50 000 \times 50 000$ entries. At a double-precision, the matrix would encompass several gigabytes of memory - far beyond the capabilities of current solvers, and most computers. Fortunately, a significant number of values in the wave matrix are $0$, causing $A$ to be highly sparse. As an example, a simulation of size $100 \times 200$ points at any discretisation would yield a wave matrix with $400,000,000$ discrete elements. Since most of these elements are $0$, the memory required to store a single sparse matrix of this type is only one megabyte, in comparison to the six gigabytes required for a full matrix.

\subsection{Convergence times}

Here, the convergence times of the frequency and time domain simulations are considered. 
\begin{figure}[t]
	\centering
	\setlength{\figH}{0.3\textwidth}
	\setlength{\figW}{0.7\textwidth}
	\input{graphs/benchmark/steadystate.tex}
	\caption[Steady state time of the $E_z$ field]{Steady state solution for the benchmark problem. A single point of the $E_z$ field is sampled at the far end of the simulation, just within the PML boundary. The simulation is observed to reach a steady state after around $2 000$ time steps (667 fs in simulation time). Deviations in subsequent oscillations are due to reflections from the PML layer.}
	\label{fig:steadystate}
\end{figure}
The difficulty in drawing benchmark comparisons between FDTD and FDFD lie in the inherently different techniques both methods use to approach a full-wave solution. FDTD operates by effectively `brute-forcing' a solution over each time-step, whereas FDFD generates and then solves a matrix equation, independent of any meaningful time-step. Naturally, the solution obtained by FDFD is dependent not only on the spatial step chosen in the simulation, but also in the number of sidebands calculated. 

To compare the speeds of time and frequency domain methods used here, the time until a steady-state for the time domain is obtained and compared to the time to convergence from the frequency domain solution. This is because the frequency domain method always yields the steady-state solution, whereas the time domain can simulate field propagation indefinitely, even after the steady-state is reached. To determine the time until the steady state fields are obtained for the time domain, a single probe is placed at the far end of the simulation, just within the boundaries of the PML layer. The probe collects a single value of the transverse electric field at every time-step. When the steady state solution is reached, the field will have no transient response as seen in Figure \ref{fig:steadystate}. Since sampling the field itself would slow down  convergence, the simulation is run twice: the first time to obtain the \textit{time step} of convergence, and the second without the probe to obtain the convergence till the given step. Convergence time in this case is simply the time (in seconds) for the complete simulation to run until the steady state. This value is different to the simulation time, which is the effective time inside the simulation. For example, it might take a single pulse $10 \mskip3mu s$ of convergence time to simulate $500\mskip3mu fs$ of simulation time. Multiple convergence times are obtained for different resolutions, from a spatial step of $0.02 \mskip3mu \mu m$ up to $1  \mskip3mu \mu m$, with an average value taken for $3$ benchmarks per spatial step. \textit{MATLAB}'s inbuilt \textit{tic} and \textit{toc} functions are used to acquire the convergence time.

\begin{figure}[t]
	\centering
	\setlength{\figH}{0.4\textwidth}
	\setlength{\figW}{0.4\textwidth}
	\begin{subfigure}[t]{0.5\textwidth}
		\input{graphs/benchmark/nomodulation/nomodbench2.tex}
		\caption{}
		\label{sfig:nomodbench}
	\end{subfigure}%
	\begin{subfigure}[t]{0.5\textwidth}
		\input{graphs/benchmark/abcavity/benchmarkabc.tex}
		\caption{}
		\label{sfig:benchabc}
	\end{subfigure}%
	\caption[Convergence time benchmarks for FDTD and FDFD]{\textbf{(a)} Benchmark of a simple $5 \times 5 \mu m$ simulation with a waveguide of $1.1 \mu m$ width extending across horizontally. Only the $N=0$ sideband is calculated for the FDFD, since no modulation occurs. \textbf{(b)} Comparison of convergence times for both the time and frequency domain simulations (with 3 sidebands) for the AB waveguide discussed in the previous chapter. The run-time was calculated by using \textit{MATLABs} inbuilt \textit{tic} and \textit{toc} functions. A smaller spatial step corresponds to a larger resolution and consequently, improved accuracy}
	\label{fig:benchmark}
\end{figure}

The first benchmark is a continuous $1  \mskip3mu \mu m$ wavelength source radiating in a $5 \times 5  \mskip3mu \mu m$ simulation space \textit{in vacuo}, with a PML layer width of $0.1  \mskip3mu \mu m$. No modulation is applied in this case, to measure the impact that continuous updates of permittivity have on the time domain simulation. Similarly, without a modulation the frequency domain solution will have $0$ sidebands to calculate over, drastically reducing the size of the wave matrix. The results of the benchmark are shown in Figure \ref{sfig:nomodbench}. For the maximum resolution, the frequency domain takes $0.75 \pm 0.01  \mskip3mu s$ to converge, over an averaged $3$ benchmarks per spatial step. The time domain takes $5.34 \pm 0.26  \mskip3mu s$ to converge, more than $7 \times $ longer than the frequency domain.

The second benchmark program is the optical AB isolator of Section \ref{sec:opticAB}. This presents two new challenges for the simulation - a much larger computational domain (exceeding $60  \mskip3mu \mu m$) as well as two regions of modulation. Again, benchmarks are taken for spatial steps $\Delta$ from $0.02  \mskip3mu \mu m$ to $1 \mu m$, with convergence times shown in Figure \ref{sfig:benchabc}. There is a much larger discrepancy between convergence times for the time and frequency methods. In fact, at the smallest spatial step of $0.02  \mskip3mu \mu m$, the frequency domain method takes $13.39 \pm 0.68 \mskip3mu  s$, whereas the time domain method exceeds $1374.70 \pm 14.91  \mskip3mu s$ (around $22.9$ minutes). For the purpose of the AB simulation then, the frequency method is just over $100 \times$ faster. Of course, these benchmarks are preliminary and would require more thorough testing under different environments. However, the ability to rapidly prototype photonic devices and then simulate is obviously an appealing aspect. As integrated and topological photonics becomes more widespread, faster simulation methods are required. Thus, FDFD is quite well suited to dynamic modulations - the number of modulated regions does not adversely affect the convergence time. On the other hand, in the time domain, each modulation region requires an additional change of the permittivity tensor at every time-step.


It is worth pointing out that even with the increase in speed FDFD brings, it is not always the ideal choice for simulating active photonics. This is because the frequency domain implementation can not calculate transient responses - that is, the final solution will always be in the steady-state. On the other hand, FDTD will always show the transient response of the system. Thus, \textit{FDFD} is ideally suited in the case when only the steady-state solution is desired.

Although the FDTD code presented here has been extensively optimised, the benchmark would also ideally be performed with a publicly available FDTD solver. The most commonly used of these, \textit{MEEP}  \cite{Oskooi2010}, does not support time-dependent media. Regardless, a preliminary test in \textit{MEEP} without modulation of the optical AB simulation showed around a $30 \%$ increase in speed over the \textit{MATLAB} FDTD code, however the act of modulating the permittivity is a costly process in of itself, and thus no reliable benchmark can be obtained. 

\subsection{Numeric artefacts}

Finite difference methods generally provide a good approximation to the real physical behaviour of fields. However, taking \textit{finite} differences of normally continuous functions will inevitably introduce an error. For instance, the group velocity of a wave propagating in the Yee grid will in general differ from $c$. This discrepancy depends on several factors including the spatial grid size, direction of propagation, and even the frequency of incident light. Such errors are known as \textit{dispersion}. Likewise, \textit{FDTD} samples the electromagnetic field at points that are discrete in both time and space. The choice of these steps is thus important for maintaining the \textit{stability} of the solution.

Another property of finite difference methods that can cause spurious results lie in the building of a material on the Yee grid. Since the Yee grid is a square lattice, it is impossible to completely place a curved structure over the grid. In every case, the structure will be represented on the grid as discrete blocks, regardless of the spatial step - an error known as staircasing. Since most of the simulations performed in this thesis were of rectangular or otherwise perfectly straight materials, staircasing will not contribute greatly to an error in the fields. Such effects can be minimised by taking an average of the surrounding permittivities, in a technique known as subpixel averaging.

